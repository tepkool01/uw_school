{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "A *Support Vector machine* (SVM) is a very powerful and versatile supervised machine learning model, capable of performing linear or nonlinear classification and regression tasks. To separate the two classes of data points, there are many possible hyperplanes that could be chosen. The key idea behind SVM is that the algorithm finds an optimal hyperplane that has the **maximum margin**, i.e the maximum distance between data points of both classes. \n",
    "\n",
    "<img src=\"svm_hyperplanes.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "This fundamental idea is explained with the plots above. The plot on the left shows many hyperplanes that can separate the circles from the squares. These hyperplanes work perfectly on this training dataset, but their decision boundaries come so close to the instances that they will probably not perform as well on new instances.  However, the right plot shows the optimal hyperplane that maximize the margin between these two classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "In this lab assignment, you will learn to use and fine-tune linear SVM, the Polynomial kernel SVM and the RBF kernel SVM, and understand their pros and cons by comparing their prediction performance on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# make this notebook's output stable across runs\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions used in this lab\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a learnt classifier\n",
    "    \"\"\"\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=1)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    \n",
    "def evaluate_model(model, X, y):\n",
    "    # make prediction and evaluate the model performance on test data\n",
    "    z = model.predict(X)\n",
    "    z_prob = model.predict_proba(X)[:,1]\n",
    "\n",
    "    print(\"model accuracy: {}\".format(accuracy_score(y, z)))\n",
    "    print(\"model ROC AUC: {}\".format(roc_auc_score(y, z_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Moon dataset\n",
    "In this lab, we will classify a moon shaped synthetic dataset with two features (_x1_ and _x2_) and two classes (0 or 1) with some added noise. We use this synthetic dataset because it's easier to help demonstrate the pros and cons of different SVM models covered in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the moon train and test data from CSV files\n",
    "train = pd.read_csv(\"moon_train.csv\")\n",
    "test = pd.read_csv(\"moon_test.csv\")\n",
    "\n",
    "train_x = train.iloc[:,0:2]\n",
    "train_y = train.iloc[:,2]\n",
    "\n",
    "test_x = test.iloc[:,0:2]\n",
    "test_y = test.iloc[:,2]\n",
    "\n",
    "print(\"Number of train data: {}\".format(len(train_y)))\n",
    "print(\"Number of test data: {}\".format(len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the train dataset\n",
    "plt.scatter(train_x.x1, train_x.x2, s=40, c=train_y, cmap=plt.cm.Spectral)\n",
    "plt.title('Scatter plot of the train dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the test dataset\n",
    "plt.scatter(test_x.x1, test_x.x2, s=40, c=test_y, cmap=plt.cm.Spectral)\n",
    "plt.title('Scatter plot of the test dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 1: Learn to use and fine-tune the Linear Support Vector Machine.**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Unlike tree-based models (e.g. CART, Random Forest and Gradient Boosted Trees), SVMs are sensitive to the feature scales. It is best practice to apply feature scaling with [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) function in scikit-learn. \n",
    "\n",
    "[Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in scikit-learn allows us to sequentially apply a list of transformations and a final estimator. For example, in the following code we first apply the feature scaling before feeding the data to train a linear SVM model. Note that you need to set the argument _probability_ to be True so that the SVC model outputs probabilities as well as the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "linear_svc_pipe = Pipeline(\n",
    "    (\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"csv\", SVC(kernel=\"linear\", probability=True, random_state=0))\n",
    "    )\n",
    ")\n",
    "linear_svc_pipe.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(linear_svc_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, a linear SVM can produce only a linear hyperplane as the decision boundary and is not capable of modeling this moon-shaped dataset and generating the decision boundaries close to the ground truth.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(linear_svc_pipe, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penalty parameter C\n",
    "\n",
    "SVM's objective function consists of two terms: the first term measures how large the margin is (we want to maximize the margin, which corresponds to minimizing $W^T W$ in the follow equation), and the second term measures the violations of the hyperplane ($\\sum_i^m \\xi^i$). This is where the hyper-parameter $C$ comes in. It is used to tradeoff between these two objectives.  \n",
    "\n",
    "\\begin{align}\n",
    "\\min_{W, b} \\frac{1}{2}W^T W & + C \\sum_i^m \\xi^i \\\\\n",
    "S.T. y^i (W^T X^i + b) & \\geqslant 1 - \\xi^i \\\\\n",
    "\\xi^i & \\geqslant 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use grid search with cross-validation on penalty parameter C to fine-tune the SVC model.\n",
    "#       Note that you need to set probability to be True so that the trained model outputs both predictions as well as probabilities.\n",
    "parameters = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "linear_svc_grid = # TODO: Use grid search with cross-validation on penalty parameter C to fine-tune the SVC model.\n",
    "\n",
    "# define the pipeline\n",
    "linear_svc_grid_pipe = # TODO: build the pipeline for training the model and fit the model to the training dataset.\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"The best score is {}\".format(linear_svc_grid.best_score_))\n",
    "print(\"The best hyper parameter setting is {}\".format(linear_svc_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(linear_svc_grid_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fine-tuning the model, we should see the model performance improve slightly as compared to the SVC with the default setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(linear_svc_grid_pipe, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel SVM\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 2: Understand the limitation of linear SVM and learn how to use kernel trick to extend linear SVM to handle non-linear scenarios.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features \n",
    "\n",
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not linearly separable. One approach to handling nonlinear datasets is to add more non-linear features, such as polynomial features, and hope the dataset is linear separable in the higher dimensional space with the help of non-linear features. \n",
    "\n",
    "In the following example, we apply polynomial functions with degree of 3 to the original features before feeding the data to the linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_feature_svm_pipe = Pipeline(\n",
    "    (\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"linear\", probability=True))\n",
    "    )\n",
    ")\n",
    "poly_feature_svm_pipe.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(poly_feature_svm_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(poly_feature_svm_pipe, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial SVM\n",
    "Instead of explicitly applying the polynomial functions to each data point, kernel trick allows us to get the same result as if you added many polynomial features (even with very high-degree polynomials) without actually having to add them to each data point. So the number of features won't grow combinatorially. \n",
    "\n",
    "To train the SVC with polynomial kernel, we need to specify argument _kernel_ to be \"poly\". Like linear SVC, we need to fine tune the penalty parameter *C* to tradeoff minimizing margin and minimizing violations. Also, we need to specify the degree of the polynomial functions as well as a hyper-parameter _coef0_ which controls how much the model is influenced by high-degree polynomials vs. low-degree polynomials. Please refer to the [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) API doc for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use grid search with cross-validation on penalty parameter C, polynomial degree, and coef0 to \n",
    "#       fine-tune the model. \n",
    "parameters = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"degree\": [3, 5],\n",
    "    \"coef0\": [0.5, 1.0]\n",
    "}\n",
    "poly_svc_grid = # TODO: Use grid search with cross-validation on hyper-parameters to fine-tune the SVC model.\n",
    "\n",
    "# define the pipeline\n",
    "poly_svc_grid_pipe = # TODO: build the pipeline for training the model and fit the model to the training dataset.\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"The best score is {}\".format(poly_svc_grid.best_score_))\n",
    "print(\"The best hyper parameter setting is {}\".format(poly_svc_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(poly_svc_grid_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(poly_svc_grid_pipe, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the polynomial SVC and adding polynomial features in a linear SVC result in very similar performance and significantly improve the model performance on the test dataset. The decision boundaries are no longer linear in the original feature space and better capture the ground truth of this moon-shaped dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Radial Basis Function (RBF) SVM \n",
    "<span style=\"color:orange\">**Coding Part 3: Learn to use Gaussian Radial Basis Function (RBF) SVM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very popular kernel used in SVM is called the Gaussian Radial Basis Function (RBF) kernel. The idea of RBF kernel is to allow each training instance to resemble a particular landmark. The RBF kernel on two samples x and x', represented as feature vectors in some input space, is defined as\n",
    "\n",
    "\\begin{align}\n",
    "K(x, x') = \\exp(-\\gamma||x - x'||^2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Kernel has a hyper-parameter $\\gamma$, which controls the shape of the Gaussian function. The parameter $\\gamma$ is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.\n",
    "\n",
    "To demonstrate this effect of hyper-parameter $\\gamma$, we train two separate SVCs, one with a large value of $\\gamma$ and one with a small value of $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train RBF SVC with a small gamma and underfit the training set\n",
    "rbf_svc_pipe = Pipeline(\n",
    "    (\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(kernel=\"rbf\", gamma=0.01, probability=True))\n",
    "    )\n",
    ")\n",
    "rbf_svc_pipe.fit(train_x, train_y)\n",
    "plot_decision_boundary(rbf_svc_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train RBF SVC with a large gamma and overfit the training set\n",
    "rbf_svc_pipe = Pipeline(\n",
    "    (\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(kernel=\"rbf\", gamma=100, probability=True))\n",
    "    )\n",
    ")\n",
    "rbf_svc_pipe.fit(train_x, train_y)\n",
    "plot_decision_boundary(rbf_svc_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the previous two plots, $\\gamma$ acts like a regularization hyper-parameter. A small value of $\\gamma$ leads to model underfitting (the upper figure) and a large value of $\\gamma$ leads to model overfitting (the lower figure). Like other hyper-parameters, the value of $\\gamma$ needs to be fine-tuned with a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use grid search with cross-validation on penalty parameter C to fine-tune the model\n",
    "parameters = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"gamma\": [0.1, 1, 10, 100]\n",
    "}\n",
    "rbf_svc_grid = # TODO: Use grid search with cross-validation on hyper-parameters to fine-tune the SVC model.\n",
    "\n",
    "# define the pipeline\n",
    "rbf_svc_grid_pipe = # TODO: build the pipeline for training the model and fit the model to the training dataset.\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"The best score is {}\".format(rbf_svc_grid.best_score_))\n",
    "print(\"The best hyper parameter setting is {}\".format(rbf_svc_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(rbf_svc_grid_pipe, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(rbf_svc_grid_pipe, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of ML 310 Lab Assignment 4\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
