{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this homework, you will apply the TFIDF technique to text classification as well as use word2vec model to generate the dense word embedding for other NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "In this lab, we will experiment different feature extraction on the 20 newgroups dataset, including the count vector and TF-IDF vector. Also, we will apply the Naive Bayes classifier  to this dataset and report the prediciton accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqbOm4jBC92H"
   },
   "source": [
    "### Load the explore the 20newsgroup data\n",
    "\n",
    "20 news group data is part of the sklearn library. We can directly load the data using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data:11314\n",
      "Number of categories:20\n",
      "From: cubbie@garnet.berkeley.edu (                               )\n",
      "Subject: Re: Cubs behind Marlins? How?\n",
      "Article-I.D.: agate.1pt592$f9a\n",
      "Organization: University of California, Berkeley\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: garnet.berkeley.edu\n",
      "\n",
      "\n",
      "gajarsky@pilot.njin.net writes:\n",
      "\n",
      "morgan and guzman will have era's 1 run higher than last year, and\n",
      " the cubs will be idiots and not pitch harkey as much as hibbard.\n",
      " castillo won't be good (i think he's a stud pitcher)\n",
      "\n",
      "       This season so far, Morgan and Guzman helped to lead the Cubs\n",
      "       at top in ERA, even better than THE rotation at Atlanta.\n",
      "       Cubs ERA at 0.056 while Braves at 0.059. We know it is early\n",
      "       in the season, we Cubs fans have learned how to enjoy the\n",
      "       short triumph while it is still there.\n",
      "\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": "['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the traning data and test data\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# print total number of categories\n",
    "print(\"Number of training data:\" + str(len(twenty_train.data)))\n",
    "print(\"Number of categories:\" + str(len(twenty_train.target_names)))\n",
    "\n",
    "# print the first text and its category\n",
    "print(twenty_train.data[0])\n",
    "print(twenty_train.target[0])\n",
    "\n",
    "# You can check the target variable by printing all the categories\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3db70c26-d684-478a-bcd4-980ed6c6d65b",
    "_uuid": "794fb768f4a8e42c4be4f1dbb27144aae4d00c79",
    "colab_type": "text",
    "id": "FfZcjrp7DWwJ"
   },
   "source": [
    "### Build a Naive Bayes Model \n",
    "\n",
    "Your task is to build practice an ML model to classify the newsgroup data into different categories. You will try both raw count and TF-IDF for feature extraction and then followed by a Naive Bayes classifier. Note that you can connect the feature generation and model training steps into one by using the [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in sklearn.\n",
    "\n",
    "Try to use Grid Search to find the best hyper parameter from the following settings (feel free to explore other options as well):\n",
    "\n",
    "* Differnet ngram range\n",
    "* Weather or not to remove the stop words\n",
    "* Weather or not to apply IDF\n",
    "\n",
    "After building the best model from the training set, we apply that model to make predictions on the test data and report its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 16 10 ...  3  3  7]\n",
      "[10 16 14 ...  4  6  7]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.8074880509824748"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"vect__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vect__stop_words\": [None, 'english'],\n",
    "    \"tfidf__use_idf\": [True, False]\n",
    "}\n",
    "text_clf = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('naive_bayes', MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "# Applying gridsearch\n",
    "grid_search = GridSearchCV(estimator=text_clf, param_grid=params)\n",
    "grid_search.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# creating predictions and comparing results\n",
    "test_predictions = grid_search.predict(twenty_test.data)\n",
    "\n",
    "np.mean(test_predictions == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding with word2vec\n",
    "\n",
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. \n",
    "\n",
    "In this assessment, we will experiment with [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) model from package [gensim](https://radimrehurek.com/gensim/) and generate word embeddings from a review dataset. You can then explore those word embeddings and see if they make sense semantically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import logging\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file reviews_data.txt.gz...this may take a while\n",
      "read 0 reviews\n",
      "read 10000 reviews\n",
      "read 20000 reviews\n",
      "read 30000 reviews\n",
      "read 40000 reviews\n",
      "read 50000 reviews\n",
      "read 60000 reviews\n",
      "read 70000 reviews\n",
      "read 80000 reviews\n",
      "read 90000 reviews\n",
      "read 100000 reviews\n",
      "read 110000 reviews\n",
      "read 120000 reviews\n",
      "read 130000 reviews\n",
      "read 140000 reviews\n",
      "read 150000 reviews\n",
      "read 160000 reviews\n",
      "read 170000 reviews\n",
      "read 180000 reviews\n",
      "read 190000 reviews\n",
      "read 200000 reviews\n",
      "read 210000 reviews\n",
      "read 220000 reviews\n",
      "read 230000 reviews\n",
      "read 240000 reviews\n",
      "read 250000 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-27 15:02:53,134 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    " \n",
    "            if (i % 10000 == 0):\n",
    "                print(\"read {0} reviews\".format(i))\n",
    "            # do some pre-processing and return list of words for each review b text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "            \n",
    "documents = list(read_input('reviews_data.txt.gz'))\n",
    "logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the word2vec model\n",
    "\n",
    "The word2vec algorithms include skip-gram and CBOW models, using either hierarchical softmax or negative sampling introduced in Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. A word2vec tutorial can be found [here](https://rare-technologies.com/word2vec-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-27 15:19:59,670 : INFO : collecting all words and their counts\n",
      "2021-02-27 15:19:59,671 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-27 15:19:59,876 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2021-02-27 15:20:00,107 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2021-02-27 15:20:00,361 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2021-02-27 15:20:00,576 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2021-02-27 15:20:00,816 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2021-02-27 15:20:01,069 : INFO : PROGRESS: at sentence #60000, processed 11013726 words, keeping 76786 word types\n",
      "2021-02-27 15:20:01,277 : INFO : PROGRESS: at sentence #70000, processed 12637528 words, keeping 83199 word types\n",
      "2021-02-27 15:20:01,483 : INFO : PROGRESS: at sentence #80000, processed 14099754 words, keeping 88459 word types\n",
      "2021-02-27 15:20:01,686 : INFO : PROGRESS: at sentence #90000, processed 15662152 words, keeping 93357 word types\n",
      "2021-02-27 15:20:01,889 : INFO : PROGRESS: at sentence #100000, processed 17164490 words, keeping 97886 word types\n",
      "2021-02-27 15:20:02,066 : INFO : PROGRESS: at sentence #110000, processed 18652295 words, keeping 102132 word types\n",
      "2021-02-27 15:20:02,252 : INFO : PROGRESS: at sentence #120000, processed 20152532 words, keeping 105923 word types\n",
      "2021-02-27 15:20:02,445 : INFO : PROGRESS: at sentence #130000, processed 21684333 words, keeping 110104 word types\n",
      "2021-02-27 15:20:02,685 : INFO : PROGRESS: at sentence #140000, processed 23330209 words, keeping 114108 word types\n",
      "2021-02-27 15:20:02,891 : INFO : PROGRESS: at sentence #150000, processed 24838757 words, keeping 118174 word types\n",
      "2021-02-27 15:20:03,104 : INFO : PROGRESS: at sentence #160000, processed 26390913 words, keeping 118670 word types\n",
      "2021-02-27 15:20:03,348 : INFO : PROGRESS: at sentence #170000, processed 27913919 words, keeping 123356 word types\n",
      "2021-02-27 15:20:03,564 : INFO : PROGRESS: at sentence #180000, processed 29535615 words, keeping 126748 word types\n",
      "2021-02-27 15:20:03,766 : INFO : PROGRESS: at sentence #190000, processed 31096462 words, keeping 129847 word types\n",
      "2021-02-27 15:20:04,004 : INFO : PROGRESS: at sentence #200000, processed 32805274 words, keeping 133255 word types\n",
      "2021-02-27 15:20:04,212 : INFO : PROGRESS: at sentence #210000, processed 34434201 words, keeping 136364 word types\n",
      "2021-02-27 15:20:04,425 : INFO : PROGRESS: at sentence #220000, processed 36083485 words, keeping 139418 word types\n",
      "2021-02-27 15:20:04,628 : INFO : PROGRESS: at sentence #230000, processed 37571765 words, keeping 142399 word types\n",
      "2021-02-27 15:20:04,848 : INFO : PROGRESS: at sentence #240000, processed 39138193 words, keeping 145232 word types\n",
      "2021-02-27 15:20:05,047 : INFO : PROGRESS: at sentence #250000, processed 40695052 words, keeping 147966 word types\n",
      "2021-02-27 15:20:05,156 : INFO : collected 150059 word types from a corpus of 41519358 raw words and 255404 sentences\n",
      "2021-02-27 15:20:05,157 : INFO : Loading a fresh vocabulary\n",
      "2021-02-27 15:20:05,876 : INFO : effective_min_count=1 retains 150059 unique words (100% of original 150059, drops 0)\n",
      "2021-02-27 15:20:05,878 : INFO : effective_min_count=1 leaves 41519358 word corpus (100% of original 41519358, drops 0)\n",
      "2021-02-27 15:20:06,275 : INFO : deleting the raw counts dictionary of 150059 items\n",
      "2021-02-27 15:20:06,280 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2021-02-27 15:20:06,280 : INFO : downsampling leaves estimated 30438951 word corpus (73.3% of prior 41519358)\n",
      "2021-02-27 15:20:06,597 : INFO : estimated required memory for 150059 words and 100 dimensions: 195076700 bytes\n",
      "2021-02-27 15:20:06,598 : INFO : resetting layer weights\n",
      "2021-02-27 15:20:31,695 : INFO : training model with 3 workers on 150059 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-27 15:20:32,706 : INFO : EPOCH 1 - PROGRESS: at 4.34% examples, 1330225 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:33,711 : INFO : EPOCH 1 - PROGRESS: at 9.01% examples, 1400648 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:34,718 : INFO : EPOCH 1 - PROGRESS: at 12.79% examples, 1403877 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:35,718 : INFO : EPOCH 1 - PROGRESS: at 17.35% examples, 1450663 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:36,719 : INFO : EPOCH 1 - PROGRESS: at 21.82% examples, 1480046 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:37,721 : INFO : EPOCH 1 - PROGRESS: at 26.30% examples, 1489393 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:38,721 : INFO : EPOCH 1 - PROGRESS: at 32.05% examples, 1502954 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:20:39,726 : INFO : EPOCH 1 - PROGRESS: at 37.40% examples, 1508842 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:20:40,735 : INFO : EPOCH 1 - PROGRESS: at 42.79% examples, 1506391 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:41,741 : INFO : EPOCH 1 - PROGRESS: at 48.05% examples, 1503069 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:20:42,744 : INFO : EPOCH 1 - PROGRESS: at 53.15% examples, 1503767 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:43,745 : INFO : EPOCH 1 - PROGRESS: at 58.02% examples, 1495542 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:20:44,745 : INFO : EPOCH 1 - PROGRESS: at 63.36% examples, 1498112 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:45,752 : INFO : EPOCH 1 - PROGRESS: at 68.69% examples, 1501487 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:46,753 : INFO : EPOCH 1 - PROGRESS: at 74.00% examples, 1506289 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:47,753 : INFO : EPOCH 1 - PROGRESS: at 78.86% examples, 1509042 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:48,754 : INFO : EPOCH 1 - PROGRESS: at 83.55% examples, 1503853 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:49,757 : INFO : EPOCH 1 - PROGRESS: at 88.37% examples, 1499118 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:50,759 : INFO : EPOCH 1 - PROGRESS: at 93.08% examples, 1492061 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:51,762 : INFO : EPOCH 1 - PROGRESS: at 98.15% examples, 1490966 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:52,086 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-27 15:20:52,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-27 15:20:52,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-27 15:20:52,095 : INFO : EPOCH - 1 : training on 41519358 raw words (30438426 effective words) took 20.4s, 1492420 effective words/s\n",
      "2021-02-27 15:20:53,104 : INFO : EPOCH 2 - PROGRESS: at 5.21% examples, 1605095 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:54,108 : INFO : EPOCH 2 - PROGRESS: at 10.10% examples, 1607115 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:55,117 : INFO : EPOCH 2 - PROGRESS: at 14.72% examples, 1617531 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:56,117 : INFO : EPOCH 2 - PROGRESS: at 19.14% examples, 1615270 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:57,118 : INFO : EPOCH 2 - PROGRESS: at 23.25% examples, 1591189 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:20:58,123 : INFO : EPOCH 2 - PROGRESS: at 28.08% examples, 1566917 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:20:59,124 : INFO : EPOCH 2 - PROGRESS: at 33.15% examples, 1546236 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:00,132 : INFO : EPOCH 2 - PROGRESS: at 38.55% examples, 1546952 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:01,134 : INFO : EPOCH 2 - PROGRESS: at 44.14% examples, 1546559 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:02,141 : INFO : EPOCH 2 - PROGRESS: at 49.16% examples, 1533248 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:03,145 : INFO : EPOCH 2 - PROGRESS: at 54.20% examples, 1531621 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:04,147 : INFO : EPOCH 2 - PROGRESS: at 59.23% examples, 1523776 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:05,151 : INFO : EPOCH 2 - PROGRESS: at 64.55% examples, 1520417 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:06,152 : INFO : EPOCH 2 - PROGRESS: at 69.58% examples, 1521242 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:07,158 : INFO : EPOCH 2 - PROGRESS: at 74.78% examples, 1521345 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:08,160 : INFO : EPOCH 2 - PROGRESS: at 79.16% examples, 1513615 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:09,166 : INFO : EPOCH 2 - PROGRESS: at 83.93% examples, 1509478 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:10,170 : INFO : EPOCH 2 - PROGRESS: at 89.43% examples, 1514359 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:11,170 : INFO : EPOCH 2 - PROGRESS: at 94.90% examples, 1518399 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:12,077 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-27 15:21:12,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-27 15:21:12,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-27 15:21:12,085 : INFO : EPOCH - 2 : training on 41519358 raw words (30437412 effective words) took 20.0s, 1522831 effective words/s\n",
      "2021-02-27 15:21:13,091 : INFO : EPOCH 3 - PROGRESS: at 4.90% examples, 1509174 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:14,099 : INFO : EPOCH 3 - PROGRESS: at 9.61% examples, 1504671 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:15,102 : INFO : EPOCH 3 - PROGRESS: at 13.60% examples, 1490272 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:16,105 : INFO : EPOCH 3 - PROGRESS: at 17.72% examples, 1487436 words/s, in_qsize 4, out_qsize 1\n",
      "2021-02-27 15:21:17,108 : INFO : EPOCH 3 - PROGRESS: at 21.80% examples, 1479032 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:18,110 : INFO : EPOCH 3 - PROGRESS: at 26.08% examples, 1479873 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:19,111 : INFO : EPOCH 3 - PROGRESS: at 31.37% examples, 1475160 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:20,125 : INFO : EPOCH 3 - PROGRESS: at 36.27% examples, 1464494 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:21,128 : INFO : EPOCH 3 - PROGRESS: at 41.25% examples, 1457169 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:22,130 : INFO : EPOCH 3 - PROGRESS: at 46.34% examples, 1452169 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:23,133 : INFO : EPOCH 3 - PROGRESS: at 51.00% examples, 1442593 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:24,137 : INFO : EPOCH 3 - PROGRESS: at 55.50% examples, 1433866 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:25,140 : INFO : EPOCH 3 - PROGRESS: at 60.56% examples, 1436308 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:26,149 : INFO : EPOCH 3 - PROGRESS: at 65.68% examples, 1437141 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:27,150 : INFO : EPOCH 3 - PROGRESS: at 70.51% examples, 1438736 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:28,151 : INFO : EPOCH 3 - PROGRESS: at 75.30% examples, 1437234 words/s, in_qsize 4, out_qsize 0\n",
      "2021-02-27 15:21:29,152 : INFO : EPOCH 3 - PROGRESS: at 79.84% examples, 1437120 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:30,154 : INFO : EPOCH 3 - PROGRESS: at 84.51% examples, 1436796 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:31,155 : INFO : EPOCH 3 - PROGRESS: at 89.40% examples, 1435003 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:32,159 : INFO : EPOCH 3 - PROGRESS: at 94.16% examples, 1432050 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:33,168 : INFO : EPOCH 3 - PROGRESS: at 99.06% examples, 1431120 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:33,344 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-27 15:21:33,347 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-27 15:21:33,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-27 15:21:33,349 : INFO : EPOCH - 3 : training on 41519358 raw words (30439374 effective words) took 21.3s, 1431666 effective words/s\n",
      "2021-02-27 15:21:34,354 : INFO : EPOCH 4 - PROGRESS: at 4.60% examples, 1423850 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:35,355 : INFO : EPOCH 4 - PROGRESS: at 9.32% examples, 1453500 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:36,358 : INFO : EPOCH 4 - PROGRESS: at 13.17% examples, 1446375 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:37,359 : INFO : EPOCH 4 - PROGRESS: at 17.32% examples, 1451771 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:38,362 : INFO : EPOCH 4 - PROGRESS: at 20.99% examples, 1443941 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:39,365 : INFO : EPOCH 4 - PROGRESS: at 24.32% examples, 1398085 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:40,366 : INFO : EPOCH 4 - PROGRESS: at 29.15% examples, 1389511 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:41,366 : INFO : EPOCH 4 - PROGRESS: at 33.62% examples, 1373988 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:42,370 : INFO : EPOCH 4 - PROGRESS: at 38.24% examples, 1368660 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:43,373 : INFO : EPOCH 4 - PROGRESS: at 43.18% examples, 1368684 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:44,374 : INFO : EPOCH 4 - PROGRESS: at 48.18% examples, 1372996 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:45,377 : INFO : EPOCH 4 - PROGRESS: at 52.65% examples, 1367624 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:46,381 : INFO : EPOCH 4 - PROGRESS: at 57.44% examples, 1368993 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:47,385 : INFO : EPOCH 4 - PROGRESS: at 62.12% examples, 1369071 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:48,386 : INFO : EPOCH 4 - PROGRESS: at 66.97% examples, 1369944 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:49,388 : INFO : EPOCH 4 - PROGRESS: at 71.81% examples, 1376682 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:50,389 : INFO : EPOCH 4 - PROGRESS: at 76.44% examples, 1377593 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:51,390 : INFO : EPOCH 4 - PROGRESS: at 80.80% examples, 1376421 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:52,395 : INFO : EPOCH 4 - PROGRESS: at 85.41% examples, 1378340 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:53,398 : INFO : EPOCH 4 - PROGRESS: at 90.04% examples, 1374015 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:54,409 : INFO : EPOCH 4 - PROGRESS: at 95.01% examples, 1377131 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:21:55,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-27 15:21:55,399 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-27 15:21:55,402 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-27 15:21:55,403 : INFO : EPOCH - 4 : training on 41519358 raw words (30441138 effective words) took 22.1s, 1380459 effective words/s\n",
      "2021-02-27 15:21:56,413 : INFO : EPOCH 5 - PROGRESS: at 4.43% examples, 1359723 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:57,418 : INFO : EPOCH 5 - PROGRESS: at 9.27% examples, 1440594 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:58,422 : INFO : EPOCH 5 - PROGRESS: at 12.89% examples, 1415492 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:21:59,424 : INFO : EPOCH 5 - PROGRESS: at 17.16% examples, 1433256 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:00,428 : INFO : EPOCH 5 - PROGRESS: at 21.10% examples, 1449193 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:01,432 : INFO : EPOCH 5 - PROGRESS: at 25.07% examples, 1433458 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:02,433 : INFO : EPOCH 5 - PROGRESS: at 29.36% examples, 1396240 words/s, in_qsize 6, out_qsize 0\n",
      "2021-02-27 15:22:03,437 : INFO : EPOCH 5 - PROGRESS: at 34.07% examples, 1389317 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:04,449 : INFO : EPOCH 5 - PROGRESS: at 38.48% examples, 1372128 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:05,454 : INFO : EPOCH 5 - PROGRESS: at 42.70% examples, 1352958 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:06,454 : INFO : EPOCH 5 - PROGRESS: at 46.93% examples, 1337086 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:07,455 : INFO : EPOCH 5 - PROGRESS: at 50.95% examples, 1321421 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:08,456 : INFO : EPOCH 5 - PROGRESS: at 55.13% examples, 1316959 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:09,459 : INFO : EPOCH 5 - PROGRESS: at 59.28% examples, 1307860 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:10,461 : INFO : EPOCH 5 - PROGRESS: at 62.85% examples, 1289582 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:11,475 : INFO : EPOCH 5 - PROGRESS: at 66.26% examples, 1268845 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:12,477 : INFO : EPOCH 5 - PROGRESS: at 69.97% examples, 1259787 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:13,482 : INFO : EPOCH 5 - PROGRESS: at 74.00% examples, 1254496 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:14,482 : INFO : EPOCH 5 - PROGRESS: at 77.72% examples, 1251528 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:15,486 : INFO : EPOCH 5 - PROGRESS: at 81.80% examples, 1251115 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:16,491 : INFO : EPOCH 5 - PROGRESS: at 85.51% examples, 1246220 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:17,498 : INFO : EPOCH 5 - PROGRESS: at 89.15% examples, 1235247 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:18,508 : INFO : EPOCH 5 - PROGRESS: at 93.15% examples, 1232001 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:19,514 : INFO : EPOCH 5 - PROGRESS: at 97.32% examples, 1230398 words/s, in_qsize 5, out_qsize 0\n",
      "2021-02-27 15:22:20,109 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-27 15:22:20,110 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-27 15:22:20,114 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-27 15:22:20,115 : INFO : EPOCH - 5 : training on 41519358 raw words (30440025 effective words) took 24.7s, 1231968 effective words/s\n",
      "2021-02-27 15:22:20,116 : INFO : training on a 207596790 raw words (152196375 effective words) took 108.4s, 1403764 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": "(152196375, 207596790)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(min_count=1)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar words for a given word\n",
    "Once the model is built, you can find interesting patterns in the model. For example, can you find the 5 most similar words to word `polite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mvy12\\pycharmprojects\\uw_school\\venv\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "2021-02-27 15:26:50,492 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('courteous', 0.9390735626220703), ('curteous', 0.9053069949150085), ('cordial', 0.8964544534683228), ('curtious', 0.8819586634635925), ('friendly', 0.8801696300506592)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: look up top 5 words similar to 'polite' using most_similar function\n",
    "# Feel free to try other words and see if it makes sense.\n",
    "ms = model.most_similar(positive='polite', topn=5)\n",
    "print(ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the word embedding by comparing their similarities\n",
    "We can also find similarity betwen two words in the embedding space. Can you find the similarities between word `great` and `good`/`horrible`, and also `dirty` and `clean`/`smelly`. Feel free to play around with the word embedding you just learnt and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82164234\n",
      "0.3833545\n",
      "0.37658313\n",
      "0.8059554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mvy12\\pycharmprojects\\uw_school\\venv\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "c:\\users\\mvy12\\pycharmprojects\\uw_school\\venv\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\mvy12\\pycharmprojects\\uw_school\\venv\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\mvy12\\pycharmprojects\\uw_school\\venv\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# TODO: find similarities between two words using similarity function\n",
    "print(model.similarity('great', 'good'))\n",
    "print(model.similarity('great', 'horrible'))\n",
    "print(model.similarity('dirty', 'clean'))\n",
    "print(model.similarity('dirty', 'smelly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conceptual Overview\n",
    "Term Frequency Inverse Document Frequency (TF-IDF) is a way to count the frequency of words, followed by finding the IDF (log(docs/docs with word)).\n",
    "The vectorizer aspect allows us to map words to numerical data, which ise useful for ML. The word2vec algorithm apparently uses a neural network to determine the synonyms. Also, like the previous vectorizer, it converts words to numerical data."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}