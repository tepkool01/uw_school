{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this homework, you will apply the TFIDF technique to text classification as well as use word2vec model to generate the dense word embedding for other NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "In this lab, we will experiment different feature extraction on the 20 newgroups dataset, including the count vector and TF-IDF vector. Also, we will apply the Naive Bayes classifier  to this dataset and report the prediciton accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqbOm4jBC92H"
   },
   "source": [
    "### Load the explore the 20newsgroup data\n",
    "\n",
    "20 news group data is part of the sklearn library. We can directly load the data using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data:11314\n",
      "Number of categories:20\n",
      "From: cubbie@garnet.berkeley.edu (                               )\n",
      "Subject: Re: Cubs behind Marlins? How?\n",
      "Article-I.D.: agate.1pt592$f9a\n",
      "Organization: University of California, Berkeley\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: garnet.berkeley.edu\n",
      "\n",
      "\n",
      "gajarsky@pilot.njin.net writes:\n",
      "\n",
      "morgan and guzman will have era's 1 run higher than last year, and\n",
      " the cubs will be idiots and not pitch harkey as much as hibbard.\n",
      " castillo won't be good (i think he's a stud pitcher)\n",
      "\n",
      "       This season so far, Morgan and Guzman helped to lead the Cubs\n",
      "       at top in ERA, even better than THE rotation at Atlanta.\n",
      "       Cubs ERA at 0.056 while Braves at 0.059. We know it is early\n",
      "       in the season, we Cubs fans have learned how to enjoy the\n",
      "       short triumph while it is still there.\n",
      "\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the traning data and test data\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# print total number of categories\n",
    "print(\"Number of training data:\" + str(len(twenty_train.data)))\n",
    "print(\"Number of categories:\" + str(len(twenty_train.target_names)))\n",
    "\n",
    "# print the first text and its category\n",
    "print(twenty_train.data[0])\n",
    "print(twenty_train.target[0])\n",
    "\n",
    "# You can check the target variable by printing all the categories\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3db70c26-d684-478a-bcd4-980ed6c6d65b",
    "_uuid": "794fb768f4a8e42c4be4f1dbb27144aae4d00c79",
    "colab_type": "text",
    "id": "FfZcjrp7DWwJ"
   },
   "source": [
    "### Build a Naive Bayes Model \n",
    "\n",
    "Your task is to build a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a Naive Bayes classifier. Note that you can connect the feature generation and model training steps into one by using the pipeline API from sklearn.\n",
    "\n",
    "Try to use Grid Search to find the best hyper parameter from the following settings (feel free to explore other options as well):\n",
    "\n",
    "* Differnet ngram range\n",
    "* Weather or not to remove the stop words\n",
    "* Weather or not to apply IDF\n",
    "\n",
    "I am intentionally make the requirement vague to encourage you to further explore different options and find the best solution. After identifying the best model, we use that model to make predictions on the test data and report its accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtPXQW_r9dXG"
   },
   "source": [
    "### Raw count fetures from text\n",
    "We can convert the raw text into a vector of counts before feeding into a ML model. In sklearn, we have a API called `CountVectorizer` to the job for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHDIxYnHHYMp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 129796)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    stop_words = 'english',\n",
    "    max_features = None,\n",
    "    ngram_range = (1, 1)\n",
    ")\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dX4S42P9-kpf"
   },
   "source": [
    "### TF-IDF fetures from text\n",
    "Similar to raw count vector. Sklearn has a API called `TfidfTransformer` which convert raw counts ot TF-IDF feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOk2dIbnHYMr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 129796)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(\n",
    "    norm = 'l2',\n",
    "    use_idf = True,\n",
    "    smooth_idf = True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q52RivRK_EtD"
   },
   "source": [
    "### Train Naive Bayes Model\n",
    "\n",
    "Given the TFIDF features for the data, we are ready to train the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Wp2woYWTHYMu"
   },
   "outputs": [],
   "source": [
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s79A10TX_Su7"
   },
   "source": [
    "### Bulid pipeline to connect all the components together\n",
    "Another way to connect both the feature generation and model training steps into one is to use the pipeline API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MHFOLToGHYMw"
   },
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer()), \n",
    "        ('tfidf', TfidfTransformer()), \n",
    "        ('clf', MultinomialNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSxZvhXQHYMy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irLXqehCzH4O"
   },
   "source": [
    "### Grid Search\n",
    "To find the best set of hyper parameters, we can apply grid search on different settings of TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False)}\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=1, cv=2)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the best mean score and the params, run the following code\n",
    "\n",
    "gs_clf.best_score_\n",
    "gs_clf.best_params_\n",
    "\n",
    "# Output for above should be: The accuracy has now increased to ~90.6% for the NB classifier (not so naive anymore! 😄)\n",
    "# and the corresponding parameters are {‘clf__alpha’: 0.01, ‘tfidf__use_idf’: True, ‘vect__ngram_range’: (1, 2)}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765400955921402"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: make predictions using NB classifier and evaluate its accuracy on the test data\n",
    "\n",
    "predicted = gs_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding with word2vec\n",
    "\n",
    "In this assessment, we will experiment with [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) model from package [gensim](https://radimrehurek.com/gensim/) and generate word embeddings from a review dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import logging\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file reviews_data.txt.gz...this may take a while\n",
      "read 0 reviews\n",
      "read 10000 reviews\n",
      "read 20000 reviews\n",
      "read 30000 reviews\n",
      "read 40000 reviews\n",
      "read 50000 reviews\n",
      "read 60000 reviews\n",
      "read 70000 reviews\n",
      "read 80000 reviews\n",
      "read 90000 reviews\n",
      "read 100000 reviews\n",
      "read 110000 reviews\n",
      "read 120000 reviews\n",
      "read 130000 reviews\n",
      "read 140000 reviews\n",
      "read 150000 reviews\n",
      "read 160000 reviews\n",
      "read 170000 reviews\n",
      "read 180000 reviews\n",
      "read 190000 reviews\n",
      "read 200000 reviews\n",
      "read 210000 reviews\n",
      "read 220000 reviews\n",
      "read 230000 reviews\n",
      "read 240000 reviews\n",
      "read 250000 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:04:50,676 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    " \n",
    "            if (i % 10000 == 0):\n",
    "                print(\"read {0} reviews\".format(i))\n",
    "            # do some pre-processing and return list of words for each review b text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "            \n",
    "documents = list(read_input('reviews_data.txt.gz'))\n",
    "logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the word2vec model\n",
    "\n",
    "The word2vec algorithms include skip-gram and CBOW models, using either hierarchical softmax or negative sampling introduced in Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. A word2vec tutorial can be found [here](https://rare-technologies.com/word2vec-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:05:23,075 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-07-30 00:05:23,079 : INFO : collecting all words and their counts\n",
      "2019-07-30 00:05:23,082 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-07-30 00:05:23,338 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2019-07-30 00:05:23,594 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2019-07-30 00:05:23,941 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2019-07-30 00:05:24,240 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2019-07-30 00:05:24,631 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2019-07-30 00:05:25,010 : INFO : PROGRESS: at sentence #60000, processed 11013726 words, keeping 76786 word types\n",
      "2019-07-30 00:05:25,297 : INFO : PROGRESS: at sentence #70000, processed 12637528 words, keeping 83199 word types\n",
      "2019-07-30 00:05:25,555 : INFO : PROGRESS: at sentence #80000, processed 14099754 words, keeping 88459 word types\n",
      "2019-07-30 00:05:25,842 : INFO : PROGRESS: at sentence #90000, processed 15662152 words, keeping 93357 word types\n",
      "2019-07-30 00:05:26,127 : INFO : PROGRESS: at sentence #100000, processed 17164490 words, keeping 97886 word types\n",
      "2019-07-30 00:05:26,434 : INFO : PROGRESS: at sentence #110000, processed 18652295 words, keeping 102132 word types\n",
      "2019-07-30 00:05:26,712 : INFO : PROGRESS: at sentence #120000, processed 20152532 words, keeping 105923 word types\n",
      "2019-07-30 00:05:26,970 : INFO : PROGRESS: at sentence #130000, processed 21684333 words, keeping 110104 word types\n",
      "2019-07-30 00:05:27,287 : INFO : PROGRESS: at sentence #140000, processed 23330209 words, keeping 114108 word types\n",
      "2019-07-30 00:05:27,594 : INFO : PROGRESS: at sentence #150000, processed 24838757 words, keeping 118174 word types\n",
      "2019-07-30 00:05:27,860 : INFO : PROGRESS: at sentence #160000, processed 26390913 words, keeping 118670 word types\n",
      "2019-07-30 00:05:28,131 : INFO : PROGRESS: at sentence #170000, processed 27913919 words, keeping 123356 word types\n",
      "2019-07-30 00:05:28,442 : INFO : PROGRESS: at sentence #180000, processed 29535615 words, keeping 126748 word types\n",
      "2019-07-30 00:05:28,697 : INFO : PROGRESS: at sentence #190000, processed 31096462 words, keeping 129847 word types\n",
      "2019-07-30 00:05:28,967 : INFO : PROGRESS: at sentence #200000, processed 32805274 words, keeping 133255 word types\n",
      "2019-07-30 00:05:29,276 : INFO : PROGRESS: at sentence #210000, processed 34434201 words, keeping 136364 word types\n",
      "2019-07-30 00:05:29,542 : INFO : PROGRESS: at sentence #220000, processed 36083485 words, keeping 139418 word types\n",
      "2019-07-30 00:05:29,780 : INFO : PROGRESS: at sentence #230000, processed 37571765 words, keeping 142399 word types\n",
      "2019-07-30 00:05:30,029 : INFO : PROGRESS: at sentence #240000, processed 39138193 words, keeping 145232 word types\n",
      "2019-07-30 00:05:30,288 : INFO : PROGRESS: at sentence #250000, processed 40695052 words, keeping 147966 word types\n",
      "2019-07-30 00:05:30,447 : INFO : collected 150059 word types from a corpus of 41519358 raw words and 255404 sentences\n",
      "2019-07-30 00:05:30,447 : INFO : Loading a fresh vocabulary\n",
      "2019-07-30 00:05:31,309 : INFO : effective_min_count=2 retains 70537 unique words (47% of original 150059, drops 79522)\n",
      "2019-07-30 00:05:31,310 : INFO : effective_min_count=2 leaves 41439836 word corpus (99% of original 41519358, drops 79522)\n",
      "2019-07-30 00:05:31,474 : INFO : deleting the raw counts dictionary of 150059 items\n",
      "2019-07-30 00:05:31,479 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-07-30 00:05:31,479 : INFO : downsampling leaves estimated 30349251 word corpus (73.2% of prior 41439836)\n",
      "2019-07-30 00:05:31,683 : INFO : estimated required memory for 70537 words and 150 dimensions: 119912900 bytes\n",
      "2019-07-30 00:05:31,684 : INFO : resetting layer weights\n",
      "2019-07-30 00:05:32,462 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-07-30 00:05:33,500 : INFO : EPOCH 1 - PROGRESS: at 2.52% examples, 777547 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:34,517 : INFO : EPOCH 1 - PROGRESS: at 5.81% examples, 881072 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:05:35,519 : INFO : EPOCH 1 - PROGRESS: at 8.79% examples, 895965 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:36,539 : INFO : EPOCH 1 - PROGRESS: at 11.29% examples, 899210 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:05:37,550 : INFO : EPOCH 1 - PROGRESS: at 14.05% examples, 915738 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:38,564 : INFO : EPOCH 1 - PROGRESS: at 16.83% examples, 921977 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:05:39,579 : INFO : EPOCH 1 - PROGRESS: at 19.17% examples, 913532 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:40,598 : INFO : EPOCH 1 - PROGRESS: at 21.67% examples, 904563 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:41,621 : INFO : EPOCH 1 - PROGRESS: at 23.86% examples, 896903 words/s, in_qsize 14, out_qsize 5\n",
      "2019-07-30 00:05:42,632 : INFO : EPOCH 1 - PROGRESS: at 26.86% examples, 895847 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:43,638 : INFO : EPOCH 1 - PROGRESS: at 30.14% examples, 896423 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:44,651 : INFO : EPOCH 1 - PROGRESS: at 33.42% examples, 895907 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:45,670 : INFO : EPOCH 1 - PROGRESS: at 36.42% examples, 892409 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:46,673 : INFO : EPOCH 1 - PROGRESS: at 39.50% examples, 891308 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:47,678 : INFO : EPOCH 1 - PROGRESS: at 42.71% examples, 891035 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:05:48,697 : INFO : EPOCH 1 - PROGRESS: at 46.05% examples, 890702 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:49,699 : INFO : EPOCH 1 - PROGRESS: at 48.31% examples, 877737 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:05:50,725 : INFO : EPOCH 1 - PROGRESS: at 51.24% examples, 874516 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:51,732 : INFO : EPOCH 1 - PROGRESS: at 54.07% examples, 873954 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:52,734 : INFO : EPOCH 1 - PROGRESS: at 57.33% examples, 876105 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:05:53,745 : INFO : EPOCH 1 - PROGRESS: at 59.87% examples, 869443 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:54,750 : INFO : EPOCH 1 - PROGRESS: at 62.52% examples, 864689 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:55,780 : INFO : EPOCH 1 - PROGRESS: at 65.63% examples, 863549 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:05:56,807 : INFO : EPOCH 1 - PROGRESS: at 68.50% examples, 862226 words/s, in_qsize 12, out_qsize 7\n",
      "2019-07-30 00:05:57,808 : INFO : EPOCH 1 - PROGRESS: at 70.99% examples, 859132 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:58,818 : INFO : EPOCH 1 - PROGRESS: at 74.19% examples, 860373 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:05:59,821 : INFO : EPOCH 1 - PROGRESS: at 76.89% examples, 860434 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:00,835 : INFO : EPOCH 1 - PROGRESS: at 79.24% examples, 855501 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:01,837 : INFO : EPOCH 1 - PROGRESS: at 81.90% examples, 854055 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:02,851 : INFO : EPOCH 1 - PROGRESS: at 84.17% examples, 848041 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:03,855 : INFO : EPOCH 1 - PROGRESS: at 86.96% examples, 847482 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:04,943 : INFO : EPOCH 1 - PROGRESS: at 89.88% examples, 844425 words/s, in_qsize 19, out_qsize 2\n",
      "2019-07-30 00:06:05,970 : INFO : EPOCH 1 - PROGRESS: at 91.78% examples, 834739 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:06,996 : INFO : EPOCH 1 - PROGRESS: at 94.60% examples, 833957 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:06:08,000 : INFO : EPOCH 1 - PROGRESS: at 97.17% examples, 831299 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:08,806 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:06:08,807 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:06:08,824 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:06:08,826 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:06:08,827 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:06:08,863 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:06:08,864 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:06:08,865 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:06:08,877 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:06:08,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:06:08,882 : INFO : EPOCH - 1 : training on 41519358 raw words (30346683 effective words) took 36.4s, 833654 effective words/s\n",
      "2019-07-30 00:06:09,911 : INFO : EPOCH 2 - PROGRESS: at 3.05% examples, 924724 words/s, in_qsize 20, out_qsize 1\n",
      "2019-07-30 00:06:10,922 : INFO : EPOCH 2 - PROGRESS: at 6.36% examples, 964447 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:11,926 : INFO : EPOCH 2 - PROGRESS: at 9.10% examples, 934472 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:12,965 : INFO : EPOCH 2 - PROGRESS: at 11.16% examples, 885007 words/s, in_qsize 20, out_qsize 2\n",
      "2019-07-30 00:06:13,968 : INFO : EPOCH 2 - PROGRESS: at 12.92% examples, 839099 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:14,978 : INFO : EPOCH 2 - PROGRESS: at 15.30% examples, 832454 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:15,982 : INFO : EPOCH 2 - PROGRESS: at 17.44% examples, 823522 words/s, in_qsize 20, out_qsize 2\n",
      "2019-07-30 00:06:16,985 : INFO : EPOCH 2 - PROGRESS: at 19.51% examples, 817999 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:17,986 : INFO : EPOCH 2 - PROGRESS: at 22.09% examples, 824728 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:19,010 : INFO : EPOCH 2 - PROGRESS: at 24.45% examples, 832195 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:20,031 : INFO : EPOCH 2 - PROGRESS: at 26.65% examples, 810441 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:21,034 : INFO : EPOCH 2 - PROGRESS: at 30.01% examples, 820667 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:22,037 : INFO : EPOCH 2 - PROGRESS: at 32.96% examples, 818460 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:23,040 : INFO : EPOCH 2 - PROGRESS: at 36.01% examples, 824839 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:24,045 : INFO : EPOCH 2 - PROGRESS: at 39.51% examples, 834752 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:25,049 : INFO : EPOCH 2 - PROGRESS: at 42.87% examples, 840759 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:26,053 : INFO : EPOCH 2 - PROGRESS: at 46.36% examples, 847102 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:27,060 : INFO : EPOCH 2 - PROGRESS: at 49.61% examples, 852034 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:28,065 : INFO : EPOCH 2 - PROGRESS: at 52.85% examples, 858022 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:29,077 : INFO : EPOCH 2 - PROGRESS: at 56.28% examples, 864033 words/s, in_qsize 19, out_qsize 3\n",
      "2019-07-30 00:06:30,079 : INFO : EPOCH 2 - PROGRESS: at 59.59% examples, 868519 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:31,085 : INFO : EPOCH 2 - PROGRESS: at 62.77% examples, 870922 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:32,100 : INFO : EPOCH 2 - PROGRESS: at 66.02% examples, 872249 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:33,103 : INFO : EPOCH 2 - PROGRESS: at 69.23% examples, 875534 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:34,104 : INFO : EPOCH 2 - PROGRESS: at 72.49% examples, 880276 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:35,109 : INFO : EPOCH 2 - PROGRESS: at 75.72% examples, 883072 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:36,116 : INFO : EPOCH 2 - PROGRESS: at 78.82% examples, 886574 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:37,136 : INFO : EPOCH 2 - PROGRESS: at 82.09% examples, 889728 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:38,144 : INFO : EPOCH 2 - PROGRESS: at 85.10% examples, 891246 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:39,170 : INFO : EPOCH 2 - PROGRESS: at 88.60% examples, 893237 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:40,186 : INFO : EPOCH 2 - PROGRESS: at 91.73% examples, 892882 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:41,189 : INFO : EPOCH 2 - PROGRESS: at 94.74% examples, 892628 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:42,197 : INFO : EPOCH 2 - PROGRESS: at 98.06% examples, 894581 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:42,700 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:06:42,723 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:06:42,730 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:06:42,734 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:06:42,743 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:06:42,750 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:06:42,751 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:06:42,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:06:42,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:06:42,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:06:42,765 : INFO : EPOCH - 2 : training on 41519358 raw words (30350299 effective words) took 33.9s, 895918 effective words/s\n",
      "2019-07-30 00:06:43,773 : INFO : EPOCH 3 - PROGRESS: at 3.08% examples, 951201 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:44,780 : INFO : EPOCH 3 - PROGRESS: at 6.21% examples, 954962 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:45,794 : INFO : EPOCH 3 - PROGRESS: at 9.09% examples, 939304 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:46,801 : INFO : EPOCH 3 - PROGRESS: at 11.61% examples, 940035 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:47,813 : INFO : EPOCH 3 - PROGRESS: at 14.36% examples, 942421 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:48,828 : INFO : EPOCH 3 - PROGRESS: at 16.99% examples, 935724 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:49,840 : INFO : EPOCH 3 - PROGRESS: at 19.61% examples, 942121 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:50,846 : INFO : EPOCH 3 - PROGRESS: at 22.30% examples, 939006 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:51,855 : INFO : EPOCH 3 - PROGRESS: at 24.64% examples, 934464 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:52,885 : INFO : EPOCH 3 - PROGRESS: at 27.84% examples, 923574 words/s, in_qsize 14, out_qsize 5\n",
      "2019-07-30 00:06:53,893 : INFO : EPOCH 3 - PROGRESS: at 30.21% examples, 901434 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:54,901 : INFO : EPOCH 3 - PROGRESS: at 32.43% examples, 875884 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:55,918 : INFO : EPOCH 3 - PROGRESS: at 35.25% examples, 871370 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:56,919 : INFO : EPOCH 3 - PROGRESS: at 38.40% examples, 872264 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:06:57,921 : INFO : EPOCH 3 - PROGRESS: at 41.27% examples, 866987 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:06:58,923 : INFO : EPOCH 3 - PROGRESS: at 44.73% examples, 871679 words/s, in_qsize 18, out_qsize 0\n",
      "2019-07-30 00:06:59,950 : INFO : EPOCH 3 - PROGRESS: at 47.50% examples, 866465 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:00,952 : INFO : EPOCH 3 - PROGRESS: at 50.19% examples, 860739 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:01,960 : INFO : EPOCH 3 - PROGRESS: at 52.44% examples, 850361 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:02,964 : INFO : EPOCH 3 - PROGRESS: at 55.00% examples, 846697 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:04,005 : INFO : EPOCH 3 - PROGRESS: at 57.58% examples, 839272 words/s, in_qsize 14, out_qsize 5\n",
      "2019-07-30 00:07:05,007 : INFO : EPOCH 3 - PROGRESS: at 60.12% examples, 834919 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:07:06,013 : INFO : EPOCH 3 - PROGRESS: at 63.22% examples, 836800 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:07,015 : INFO : EPOCH 3 - PROGRESS: at 65.90% examples, 833941 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:08,024 : INFO : EPOCH 3 - PROGRESS: at 68.47% examples, 830445 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:09,036 : INFO : EPOCH 3 - PROGRESS: at 70.98% examples, 828620 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:10,067 : INFO : EPOCH 3 - PROGRESS: at 74.08% examples, 829280 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:11,070 : INFO : EPOCH 3 - PROGRESS: at 77.02% examples, 832993 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:12,070 : INFO : EPOCH 3 - PROGRESS: at 80.01% examples, 836247 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:13,072 : INFO : EPOCH 3 - PROGRESS: at 83.02% examples, 838568 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:14,073 : INFO : EPOCH 3 - PROGRESS: at 85.98% examples, 841122 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:15,088 : INFO : EPOCH 3 - PROGRESS: at 89.38% examples, 843973 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:16,101 : INFO : EPOCH 3 - PROGRESS: at 92.57% examples, 845824 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:17,109 : INFO : EPOCH 3 - PROGRESS: at 95.62% examples, 846870 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:18,118 : INFO : EPOCH 3 - PROGRESS: at 98.87% examples, 849579 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:18,432 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:07:18,440 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:07:18,441 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:07:18,443 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:07:18,443 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:07:18,453 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:07:18,462 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:07:18,469 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:07:18,470 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:07:18,473 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:07:18,474 : INFO : EPOCH - 3 : training on 41519358 raw words (30349977 effective words) took 35.7s, 850101 effective words/s\n",
      "2019-07-30 00:07:19,507 : INFO : EPOCH 4 - PROGRESS: at 2.80% examples, 856039 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:20,507 : INFO : EPOCH 4 - PROGRESS: at 5.40% examples, 824801 words/s, in_qsize 20, out_qsize 0\n",
      "2019-07-30 00:07:21,508 : INFO : EPOCH 4 - PROGRESS: at 7.68% examples, 787299 words/s, in_qsize 20, out_qsize 0\n",
      "2019-07-30 00:07:22,513 : INFO : EPOCH 4 - PROGRESS: at 9.90% examples, 783492 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:23,536 : INFO : EPOCH 4 - PROGRESS: at 11.92% examples, 772888 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:24,540 : INFO : EPOCH 4 - PROGRESS: at 13.84% examples, 756433 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:25,543 : INFO : EPOCH 4 - PROGRESS: at 16.08% examples, 756358 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:26,543 : INFO : EPOCH 4 - PROGRESS: at 17.89% examples, 745612 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:27,556 : INFO : EPOCH 4 - PROGRESS: at 19.73% examples, 739673 words/s, in_qsize 20, out_qsize 1\n",
      "2019-07-30 00:07:28,563 : INFO : EPOCH 4 - PROGRESS: at 21.71% examples, 730286 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:29,563 : INFO : EPOCH 4 - PROGRESS: at 23.45% examples, 725507 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:30,566 : INFO : EPOCH 4 - PROGRESS: at 25.18% examples, 714667 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:31,573 : INFO : EPOCH 4 - PROGRESS: at 28.17% examples, 720984 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:07:32,575 : INFO : EPOCH 4 - PROGRESS: at 31.09% examples, 727639 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:33,603 : INFO : EPOCH 4 - PROGRESS: at 34.49% examples, 742540 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:34,640 : INFO : EPOCH 4 - PROGRESS: at 37.82% examples, 754712 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:35,648 : INFO : EPOCH 4 - PROGRESS: at 41.12% examples, 763286 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:36,678 : INFO : EPOCH 4 - PROGRESS: at 44.53% examples, 771200 words/s, in_qsize 16, out_qsize 3\n",
      "2019-07-30 00:07:37,680 : INFO : EPOCH 4 - PROGRESS: at 47.86% examples, 780874 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:38,691 : INFO : EPOCH 4 - PROGRESS: at 50.77% examples, 782830 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:39,704 : INFO : EPOCH 4 - PROGRESS: at 53.51% examples, 785378 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:40,719 : INFO : EPOCH 4 - PROGRESS: at 56.47% examples, 787231 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:41,724 : INFO : EPOCH 4 - PROGRESS: at 59.30% examples, 788643 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:42,726 : INFO : EPOCH 4 - PROGRESS: at 62.12% examples, 790141 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:43,762 : INFO : EPOCH 4 - PROGRESS: at 65.33% examples, 792256 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:44,762 : INFO : EPOCH 4 - PROGRESS: at 68.41% examples, 797291 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:45,771 : INFO : EPOCH 4 - PROGRESS: at 70.97% examples, 797359 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:46,775 : INFO : EPOCH 4 - PROGRESS: at 73.84% examples, 797355 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:07:47,778 : INFO : EPOCH 4 - PROGRESS: at 76.30% examples, 797107 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:48,791 : INFO : EPOCH 4 - PROGRESS: at 79.28% examples, 800845 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:49,794 : INFO : EPOCH 4 - PROGRESS: at 82.21% examples, 803950 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:50,795 : INFO : EPOCH 4 - PROGRESS: at 85.11% examples, 807073 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:51,859 : INFO : EPOCH 4 - PROGRESS: at 88.24% examples, 807688 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:07:52,875 : INFO : EPOCH 4 - PROGRESS: at 91.03% examples, 806745 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:53,921 : INFO : EPOCH 4 - PROGRESS: at 93.87% examples, 806539 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:54,934 : INFO : EPOCH 4 - PROGRESS: at 97.09% examples, 809847 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:07:55,876 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:07:55,892 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:07:55,903 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:07:55,912 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:07:55,914 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:07:55,916 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:07:55,927 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:07:55,936 : INFO : EPOCH 4 - PROGRESS: at 99.95% examples, 810046 words/s, in_qsize 2, out_qsize 1\n",
      "2019-07-30 00:07:55,937 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:07:55,938 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:07:55,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:07:55,945 : INFO : EPOCH - 4 : training on 41519358 raw words (30349227 effective words) took 37.5s, 810224 effective words/s\n",
      "2019-07-30 00:07:56,974 : INFO : EPOCH 5 - PROGRESS: at 2.64% examples, 813845 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:57,977 : INFO : EPOCH 5 - PROGRESS: at 5.60% examples, 852045 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:07:58,978 : INFO : EPOCH 5 - PROGRESS: at 8.61% examples, 879181 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:00,020 : INFO : EPOCH 5 - PROGRESS: at 10.73% examples, 846676 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:08:01,031 : INFO : EPOCH 5 - PROGRESS: at 12.40% examples, 805064 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:02,056 : INFO : EPOCH 5 - PROGRESS: at 14.78% examples, 802156 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:03,059 : INFO : EPOCH 5 - PROGRESS: at 17.44% examples, 822000 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:04,083 : INFO : EPOCH 5 - PROGRESS: at 19.79% examples, 827897 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:05,085 : INFO : EPOCH 5 - PROGRESS: at 22.46% examples, 836665 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:06,093 : INFO : EPOCH 5 - PROGRESS: at 24.58% examples, 835791 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:08:07,093 : INFO : EPOCH 5 - PROGRESS: at 27.98% examples, 842405 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:08,144 : INFO : EPOCH 5 - PROGRESS: at 30.83% examples, 836115 words/s, in_qsize 16, out_qsize 3\n",
      "2019-07-30 00:08:09,161 : INFO : EPOCH 5 - PROGRESS: at 33.75% examples, 834499 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:10,162 : INFO : EPOCH 5 - PROGRESS: at 36.64% examples, 833147 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:11,184 : INFO : EPOCH 5 - PROGRESS: at 39.46% examples, 829724 words/s, in_qsize 19, out_qsize 1\n",
      "2019-07-30 00:08:12,189 : INFO : EPOCH 5 - PROGRESS: at 42.55% examples, 831132 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:13,190 : INFO : EPOCH 5 - PROGRESS: at 45.08% examples, 822213 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:08:14,195 : INFO : EPOCH 5 - PROGRESS: at 47.77% examples, 819970 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:15,200 : INFO : EPOCH 5 - PROGRESS: at 50.48% examples, 817173 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:16,203 : INFO : EPOCH 5 - PROGRESS: at 52.79% examples, 811445 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:17,203 : INFO : EPOCH 5 - PROGRESS: at 55.19% examples, 806561 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:18,212 : INFO : EPOCH 5 - PROGRESS: at 58.22% examples, 809299 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:19,223 : INFO : EPOCH 5 - PROGRESS: at 60.53% examples, 802825 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:20,224 : INFO : EPOCH 5 - PROGRESS: at 62.56% examples, 794137 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:21,234 : INFO : EPOCH 5 - PROGRESS: at 64.91% examples, 786608 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:22,234 : INFO : EPOCH 5 - PROGRESS: at 66.56% examples, 776598 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:23,327 : INFO : EPOCH 5 - PROGRESS: at 68.35% examples, 764790 words/s, in_qsize 18, out_qsize 2\n",
      "2019-07-30 00:08:24,346 : INFO : EPOCH 5 - PROGRESS: at 69.95% examples, 754815 words/s, in_qsize 17, out_qsize 4\n",
      "2019-07-30 00:08:25,348 : INFO : EPOCH 5 - PROGRESS: at 71.31% examples, 743579 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:26,361 : INFO : EPOCH 5 - PROGRESS: at 73.61% examples, 739888 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:27,371 : INFO : EPOCH 5 - PROGRESS: at 76.13% examples, 741317 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:28,380 : INFO : EPOCH 5 - PROGRESS: at 78.52% examples, 741573 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:29,390 : INFO : EPOCH 5 - PROGRESS: at 81.56% examples, 747148 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:30,397 : INFO : EPOCH 5 - PROGRESS: at 84.70% examples, 753295 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:31,427 : INFO : EPOCH 5 - PROGRESS: at 88.01% examples, 757839 words/s, in_qsize 20, out_qsize 2\n",
      "2019-07-30 00:08:32,453 : INFO : EPOCH 5 - PROGRESS: at 90.87% examples, 758908 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:33,473 : INFO : EPOCH 5 - PROGRESS: at 92.67% examples, 752122 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:34,504 : INFO : EPOCH 5 - PROGRESS: at 94.88% examples, 749060 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:35,505 : INFO : EPOCH 5 - PROGRESS: at 97.84% examples, 751769 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:36,149 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:08:36,150 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:08:36,165 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:08:36,166 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:08:36,180 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:08:36,183 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:08:36,191 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:08:36,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:08:36,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:08:36,208 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:08:36,209 : INFO : EPOCH - 5 : training on 41519358 raw words (30349612 effective words) took 40.3s, 753952 effective words/s\n",
      "2019-07-30 00:08:36,210 : INFO : training on a 207596790 raw words (151745798 effective words) took 183.7s, 825850 effective words/s\n",
      "2019-07-30 00:08:36,211 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-07-30 00:08:36,212 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-07-30 00:08:37,228 : INFO : EPOCH 1 - PROGRESS: at 2.93% examples, 907669 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:38,234 : INFO : EPOCH 1 - PROGRESS: at 5.79% examples, 890638 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:39,235 : INFO : EPOCH 1 - PROGRESS: at 8.60% examples, 883528 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:40,242 : INFO : EPOCH 1 - PROGRESS: at 10.90% examples, 873183 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:41,250 : INFO : EPOCH 1 - PROGRESS: at 12.41% examples, 813833 words/s, in_qsize 16, out_qsize 3\n",
      "2019-07-30 00:08:42,267 : INFO : EPOCH 1 - PROGRESS: at 13.96% examples, 763794 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:43,272 : INFO : EPOCH 1 - PROGRESS: at 15.67% examples, 734832 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:44,277 : INFO : EPOCH 1 - PROGRESS: at 17.25% examples, 716564 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:45,358 : INFO : EPOCH 1 - PROGRESS: at 19.18% examples, 710139 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:46,358 : INFO : EPOCH 1 - PROGRESS: at 20.51% examples, 691738 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:47,359 : INFO : EPOCH 1 - PROGRESS: at 21.74% examples, 661577 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:48,415 : INFO : EPOCH 1 - PROGRESS: at 23.07% examples, 646793 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:08:49,426 : INFO : EPOCH 1 - PROGRESS: at 25.12% examples, 652273 words/s, in_qsize 16, out_qsize 3\n",
      "2019-07-30 00:08:50,434 : INFO : EPOCH 1 - PROGRESS: at 27.63% examples, 653785 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:51,485 : INFO : EPOCH 1 - PROGRESS: at 29.91% examples, 650748 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:08:52,507 : INFO : EPOCH 1 - PROGRESS: at 31.87% examples, 641873 words/s, in_qsize 20, out_qsize 2\n",
      "2019-07-30 00:08:53,509 : INFO : EPOCH 1 - PROGRESS: at 33.50% examples, 632167 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:54,530 : INFO : EPOCH 1 - PROGRESS: at 35.09% examples, 623412 words/s, in_qsize 20, out_qsize 2\n",
      "2019-07-30 00:08:55,534 : INFO : EPOCH 1 - PROGRESS: at 37.23% examples, 622734 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:56,540 : INFO : EPOCH 1 - PROGRESS: at 39.44% examples, 621663 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:08:57,541 : INFO : EPOCH 1 - PROGRESS: at 41.86% examples, 623233 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:58,542 : INFO : EPOCH 1 - PROGRESS: at 44.29% examples, 625617 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:08:59,553 : INFO : EPOCH 1 - PROGRESS: at 46.72% examples, 627850 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:00,569 : INFO : EPOCH 1 - PROGRESS: at 49.83% examples, 638654 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:01,571 : INFO : EPOCH 1 - PROGRESS: at 52.64% examples, 646281 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:09:02,577 : INFO : EPOCH 1 - PROGRESS: at 55.34% examples, 651786 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:03,580 : INFO : EPOCH 1 - PROGRESS: at 57.91% examples, 655364 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:09:04,602 : INFO : EPOCH 1 - PROGRESS: at 60.81% examples, 660891 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:05,607 : INFO : EPOCH 1 - PROGRESS: at 64.14% examples, 669738 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:06,610 : INFO : EPOCH 1 - PROGRESS: at 67.34% examples, 679053 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:07,620 : INFO : EPOCH 1 - PROGRESS: at 70.09% examples, 684239 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:08,631 : INFO : EPOCH 1 - PROGRESS: at 71.36% examples, 674740 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:11,140 : INFO : EPOCH 1 - PROGRESS: at 72.12% examples, 632853 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:12,884 : INFO : EPOCH 1 - PROGRESS: at 72.46% examples, 605099 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:13,905 : INFO : EPOCH 1 - PROGRESS: at 72.86% examples, 591758 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:15,329 : INFO : EPOCH 1 - PROGRESS: at 73.17% examples, 572415 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:16,336 : INFO : EPOCH 1 - PROGRESS: at 73.43% examples, 559828 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:17,426 : INFO : EPOCH 1 - PROGRESS: at 74.11% examples, 549572 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:09:18,455 : INFO : EPOCH 1 - PROGRESS: at 76.05% examples, 550860 words/s, in_qsize 16, out_qsize 3\n",
      "2019-07-30 00:09:19,459 : INFO : EPOCH 1 - PROGRESS: at 76.97% examples, 544727 words/s, in_qsize 19, out_qsize 5\n",
      "2019-07-30 00:09:20,490 : INFO : EPOCH 1 - PROGRESS: at 79.69% examples, 551216 words/s, in_qsize 20, out_qsize 2\n",
      "2019-07-30 00:09:21,496 : INFO : EPOCH 1 - PROGRESS: at 82.36% examples, 556929 words/s, in_qsize 18, out_qsize 2\n",
      "2019-07-30 00:09:22,509 : INFO : EPOCH 1 - PROGRESS: at 85.37% examples, 565071 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:23,514 : INFO : EPOCH 1 - PROGRESS: at 88.73% examples, 572751 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:24,532 : INFO : EPOCH 1 - PROGRESS: at 91.66% examples, 578032 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:25,545 : INFO : EPOCH 1 - PROGRESS: at 94.19% examples, 581220 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:26,558 : INFO : EPOCH 1 - PROGRESS: at 97.35% examples, 587715 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:27,313 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:09:27,314 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:09:27,326 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:09:27,345 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:09:27,346 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:09:27,358 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:09:27,360 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:09:27,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:09:27,373 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:09:27,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:09:27,377 : INFO : EPOCH - 1 : training on 41519358 raw words (30348517 effective words) took 51.1s, 593326 effective words/s\n",
      "2019-07-30 00:09:28,400 : INFO : EPOCH 2 - PROGRESS: at 2.92% examples, 900207 words/s, in_qsize 20, out_qsize 1\n",
      "2019-07-30 00:09:29,418 : INFO : EPOCH 2 - PROGRESS: at 5.85% examples, 888665 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:30,427 : INFO : EPOCH 2 - PROGRESS: at 8.91% examples, 912844 words/s, in_qsize 20, out_qsize 0\n",
      "2019-07-30 00:09:31,432 : INFO : EPOCH 2 - PROGRESS: at 11.34% examples, 908072 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:32,439 : INFO : EPOCH 2 - PROGRESS: at 13.73% examples, 895211 words/s, in_qsize 20, out_qsize 0\n",
      "2019-07-30 00:09:33,449 : INFO : EPOCH 2 - PROGRESS: at 16.43% examples, 901806 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:34,454 : INFO : EPOCH 2 - PROGRESS: at 18.78% examples, 895751 words/s, in_qsize 20, out_qsize 0\n",
      "2019-07-30 00:09:35,457 : INFO : EPOCH 2 - PROGRESS: at 21.06% examples, 897183 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:09:36,468 : INFO : EPOCH 2 - PROGRESS: at 23.68% examples, 894483 words/s, in_qsize 13, out_qsize 6\n",
      "2019-07-30 00:09:37,474 : INFO : EPOCH 2 - PROGRESS: at 26.68% examples, 895997 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:09:38,534 : INFO : EPOCH 2 - PROGRESS: at 29.43% examples, 878691 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:39,541 : INFO : EPOCH 2 - PROGRESS: at 32.45% examples, 874140 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:40,559 : INFO : EPOCH 2 - PROGRESS: at 35.61% examples, 877391 words/s, in_qsize 19, out_qsize 1\n",
      "2019-07-30 00:09:41,585 : INFO : EPOCH 2 - PROGRESS: at 38.96% examples, 880359 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:42,589 : INFO : EPOCH 2 - PROGRESS: at 42.21% examples, 880537 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:43,605 : INFO : EPOCH 2 - PROGRESS: at 45.59% examples, 883233 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:44,613 : INFO : EPOCH 2 - PROGRESS: at 48.92% examples, 887204 words/s, in_qsize 20, out_qsize 0\n",
      "2019-07-30 00:09:45,621 : INFO : EPOCH 2 - PROGRESS: at 52.08% examples, 889005 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:46,637 : INFO : EPOCH 2 - PROGRESS: at 55.31% examples, 891960 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:09:47,639 : INFO : EPOCH 2 - PROGRESS: at 58.72% examples, 896726 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:48,644 : INFO : EPOCH 2 - PROGRESS: at 61.96% examples, 898693 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:49,646 : INFO : EPOCH 2 - PROGRESS: at 65.51% examples, 902265 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:50,647 : INFO : EPOCH 2 - PROGRESS: at 68.82% examples, 906032 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:51,649 : INFO : EPOCH 2 - PROGRESS: at 71.97% examples, 909335 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:52,657 : INFO : EPOCH 2 - PROGRESS: at 75.33% examples, 911216 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:53,668 : INFO : EPOCH 2 - PROGRESS: at 78.28% examples, 912303 words/s, in_qsize 14, out_qsize 5\n",
      "2019-07-30 00:09:54,683 : INFO : EPOCH 2 - PROGRESS: at 81.59% examples, 915574 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:55,689 : INFO : EPOCH 2 - PROGRESS: at 84.73% examples, 917084 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:56,693 : INFO : EPOCH 2 - PROGRESS: at 88.09% examples, 918344 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:57,696 : INFO : EPOCH 2 - PROGRESS: at 91.51% examples, 919931 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:09:58,711 : INFO : EPOCH 2 - PROGRESS: at 94.88% examples, 921918 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:09:59,732 : INFO : EPOCH 2 - PROGRESS: at 98.28% examples, 923306 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:00,184 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:10:00,185 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:10:00,186 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:10:00,229 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:10:00,242 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:10:00,257 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:10:00,263 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:10:00,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:10:00,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:10:00,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:10:00,274 : INFO : EPOCH - 2 : training on 41519358 raw words (30348702 effective words) took 32.9s, 922894 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:10:01,296 : INFO : EPOCH 3 - PROGRESS: at 2.84% examples, 880617 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:02,311 : INFO : EPOCH 3 - PROGRESS: at 6.05% examples, 923536 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:03,321 : INFO : EPOCH 3 - PROGRESS: at 8.82% examples, 902782 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:04,332 : INFO : EPOCH 3 - PROGRESS: at 11.45% examples, 918782 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:05,340 : INFO : EPOCH 3 - PROGRESS: at 14.20% examples, 929096 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:06,351 : INFO : EPOCH 3 - PROGRESS: at 17.04% examples, 938424 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:07,354 : INFO : EPOCH 3 - PROGRESS: at 19.61% examples, 942431 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:08,371 : INFO : EPOCH 3 - PROGRESS: at 22.40% examples, 942629 words/s, in_qsize 19, out_qsize 2\n",
      "2019-07-30 00:10:09,373 : INFO : EPOCH 3 - PROGRESS: at 24.86% examples, 940841 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:10,374 : INFO : EPOCH 3 - PROGRESS: at 28.41% examples, 942695 words/s, in_qsize 17, out_qsize 2\n",
      "2019-07-30 00:10:11,375 : INFO : EPOCH 3 - PROGRESS: at 31.97% examples, 946036 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:12,387 : INFO : EPOCH 3 - PROGRESS: at 35.10% examples, 943882 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:13,401 : INFO : EPOCH 3 - PROGRESS: at 38.52% examples, 943891 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:14,402 : INFO : EPOCH 3 - PROGRESS: at 41.88% examples, 941432 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:15,404 : INFO : EPOCH 3 - PROGRESS: at 45.53% examples, 945705 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:16,408 : INFO : EPOCH 3 - PROGRESS: at 48.78% examples, 945873 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:17,413 : INFO : EPOCH 3 - PROGRESS: at 52.11% examples, 947020 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:18,415 : INFO : EPOCH 3 - PROGRESS: at 55.41% examples, 948411 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:19,419 : INFO : EPOCH 3 - PROGRESS: at 58.83% examples, 950746 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:20,432 : INFO : EPOCH 3 - PROGRESS: at 62.10% examples, 950447 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:21,434 : INFO : EPOCH 3 - PROGRESS: at 65.63% examples, 951709 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:22,435 : INFO : EPOCH 3 - PROGRESS: at 68.90% examples, 952508 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:23,438 : INFO : EPOCH 3 - PROGRESS: at 71.90% examples, 951991 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:24,441 : INFO : EPOCH 3 - PROGRESS: at 75.29% examples, 952964 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:25,443 : INFO : EPOCH 3 - PROGRESS: at 78.33% examples, 953659 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:26,444 : INFO : EPOCH 3 - PROGRESS: at 81.49% examples, 954332 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:27,454 : INFO : EPOCH 3 - PROGRESS: at 84.72% examples, 955360 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:28,476 : INFO : EPOCH 3 - PROGRESS: at 88.21% examples, 955951 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:29,480 : INFO : EPOCH 3 - PROGRESS: at 91.48% examples, 954798 words/s, in_qsize 18, out_qsize 1\n",
      "2019-07-30 00:10:30,488 : INFO : EPOCH 3 - PROGRESS: at 94.70% examples, 954492 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:31,503 : INFO : EPOCH 3 - PROGRESS: at 98.08% examples, 954821 words/s, in_qsize 19, out_qsize 0\n",
      "2019-07-30 00:10:32,005 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-07-30 00:10:32,026 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-07-30 00:10:32,027 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-07-30 00:10:32,051 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-07-30 00:10:32,054 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-07-30 00:10:32,062 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-07-30 00:10:32,066 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-07-30 00:10:32,073 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-30 00:10:32,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-30 00:10:32,087 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-30 00:10:32,088 : INFO : EPOCH - 3 : training on 41519358 raw words (30351151 effective words) took 31.8s, 954437 effective words/s\n",
      "2019-07-30 00:10:32,088 : INFO : training on a 124558074 raw words (91048370 effective words) took 115.9s, 785746 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91048370, 124558074)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(\n",
    "    documents,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)\n",
    "model.train(documents, total_examples=len(documents), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar words for a given word\n",
    "Once the model is built, you can find interesting patterns in the model. For example, can you find the 5 most similar words to word `polite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-30 00:10:32,108 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9084916114807129),\n",
       " ('cordial', 0.8302523493766785),\n",
       " ('friendly', 0.8226535320281982),\n",
       " ('curteous', 0.8103367686271667),\n",
       " ('courtious', 0.7951087355613708)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: look up top 5 words similar to 'polite' using most_similar function\n",
    "model.wv.most_similar(positive=[\"polite\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the word embedding by comparing their similarities\n",
    "We can also find similarity betwen two words in the embedding space. Can you find the similarities between word `great` and `good`/`horrible`, and also `dirty` and `clean`/`smelly`. Feel free to play around with the word embedding you just learnt and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81034255\n",
      "0.30029735\n",
      "0.76013786\n",
      "0.267225\n"
     ]
    }
   ],
   "source": [
    "# TODO: find similarities between two words using similarity function\n",
    "print(model.wv.similarity(w1=\"great\", w2=\"good\"))\n",
    "print(model.wv.similarity(w1=\"great\", w2=\"horrible\"))\n",
    "print(model.wv.similarity(w1=\"dirty\", w2=\"smelly\"))\n",
    "print(model.wv.similarity(w1=\"dirty\", w2=\"clean\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
