{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "\n",
    "In this lab, we will use a python package named [Surprise](http://surpriselib.com/), which is an easy-to-use Python scikit for recommendation systems. It includes several commonly used algorithms, including [collaborative filtering](https://surprise.readthedocs.io/en/stable/knn_inspired.html) and [Matrix Factorization-based algorithms](https://surprise.readthedocs.io/en/stable/matrix_factorization.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # install packages\n",
    "# import sys\n",
    "\n",
    "# !pip3 install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-9e4ded65135e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msurprise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprediction_algorithms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmatrix_factorization\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSVD\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msurprise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprediction_algorithms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mknns\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mKNNBasic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msurprise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprediction_algorithms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mknns\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mKNNWithMeans\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msurprise\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprediction_algorithms\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mknns\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mKNNBaseline\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msurprise\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "from surprise.prediction_algorithms.knns import KNNWithMeans\n",
    "from surprise.prediction_algorithms.knns import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from package surprise \n",
    "\n",
    "First, we can download the ml-100k dataset included in package surprise. The data will be saved in the .surprise_data folder in your home directory. Use the API in the package to sample random trainset and testset where test set is made of 20% of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the movielens-100k dataset (download it if needed) and split the data into \n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# sample random trainset and testset where test set is made of 20% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of users: {}\".format(trainset.n_users))\n",
    "print(\"Number of items: {}\".format(trainset.n_items))\n",
    "print(\"Number of ratings: {}\".format(trainset.n_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "First, we will apply three different flavors of collaborative filtering to this data and evaluate their performances using RMSE and MAE. For each of these algorithms, the actual number of neighbors that are aggregated to compute an estimation is necessarily less than or equal to `ùëò`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic collaborative filtering algorithm\n",
    "\n",
    "**TODO**: You will study the [KNNBasic](https://surprise.readthedocs.io/en/stable/knn_inspired.html) API, choose the number of neighbors and the similarity measure, train the model based on training dataset and make predictions on the test dataset. Finally, you will evaluate the model performance based on RMSE and MAE. \n",
    "\n",
    "Try to play around with the different number of neighbors in the algorithm as well as the different similarity measure and see how it impacts the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the basic collaborative filtering algorithm.\n",
    "# See https://surprise.readthedocs.io/en/stable/knn_inspired.html for more details.\n",
    "# Sim options\n",
    "sim_1 = {'name': 'cosine', 'user_based': False} # 81.6% MAE, 1.033 RMSE\n",
    "sim_2 = {'name': 'pearson_baseline', 'shrinkage': 0} # 79.8% MAE, 1.009 RMSE\n",
    "model = KNNBasic(k=50, min_k=1, sim_options=sim_1, verbose=True)\n",
    "preds = model.fit(trainset).test(testset)\n",
    "print(\"MAE: \", accuracy.mae(preds))\n",
    "print(\"RMSE: \", accuracy.rmse(preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic collaborative filtering algorithm with user mean ratings\n",
    "\n",
    "**TODO**: A variation of the basic CF model is to take into account the mean ratings of each user. You will study the [KNNWithMeans](https://surprise.readthedocs.io/en/stable/knn_inspired.html) API, choose the number of neighbors and the similarity measure, train the model based on training dataset and make predictions on the test dataset. Finally, you will evaluate the model performance based on RMSE and MAE. \n",
    "\n",
    "Try to play around with the different number of neighbors in the algorithm as well as the different similarity measure and see how it impacts the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the basic collaborative filtering algorithm, taking into account the mean ratings of each user.\n",
    "# See https://surprise.readthedocs.io/en/stable/knn_inspired.html for more details.\n",
    "\n",
    "sim_1 = {'name': 'cosine', 'user_based': False} # 0.7415 MAE; 0.9464 RMSE\n",
    "sim_2 = {'name': 'pearson_baseline', 'shrinkage': 0} # 0.7482 MAE; 0.9560 RMSE\n",
    "model = KNNWithMeans(k=50, min_k=1, sim_options=sim_1, verbose=True)\n",
    "preds = model.fit(trainset).test(testset)\n",
    "print(\"MAE: \", accuracy.mae(preds))\n",
    "print(\"RMSE: \", accuracy.rmse(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "Then, we will explore the matrix factorization techniques for recommendation. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices. The famous SVD algorithm for matrix factorization is popularized by Simon Funk during the Netflix Prize. \n",
    "\n",
    "**TODO**: in this task, you will use the famous SVD algorithm for the implementation of the matrix factorization modeo. You will study the [SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) API, choose the number of neighbors and the similarity measure, train the model based on training dataset and make predictions on the test dataset. Finally, you will evaluate the model performance based on RMSE and MAE. \n",
    "\n",
    "Try to play around with different number of factors and also try the [SVD++ algorithm](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) and [Non-negative Matrix Factorization](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) to see if you can imporve the model preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll use the famous SVD algorithm.\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVDpp\n",
    "from surprise.prediction_algorithms.matrix_factorization import NMF\n",
    "\n",
    "model = SVD(n_factors=50) # Default factors (20) was 0.9423 RMSE\n",
    "cross_validate(model, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n",
    "\n",
    "ppmodel = SVDpp(n_factors=50) # Default factors (20) was 0.92 RMSE\n",
    "cross_validate(ppmodel, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n",
    "\n",
    "nmfmodel = NMF(n_factors=50) # Default factors (15) was 0.9788 RMSE\n",
    "cross_validate(nmfmodel, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BONUS] \n",
    "Implement your own version of User-User or Item-Item Collaborative Filtering and compare its performance against the surprise package's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Lab: Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conceptual Overview\n",
    "KNN uses a design of nearest-neighbor design which says points on a graph are likely to be related/similar to points they are close to.\n",
    "Determining the relative closeness of the neighbors can be derived different ways (pearson, cosine, etc). The KNNMeans smooths out the predictions by leveraging the average of the user ratings/reviews/etc as part of the similarity equation.\n",
    "SVD uses gradient descent to reduce the error, and fills in missing values in the matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}