{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Ensemble learning helps improve machine learning results by combining several base models. This approach often leads to better predictive performance compared to a single base model alone. Ensemble learning has been proved to be very effective and used extensively in many machine learning competitions, such as the Netflix prize competition and Kaggle.\n",
    "\n",
    "Like bagging (Random Forest) and boosting (Gradient Boosted Trees) covered in the last lab, stacked generalization (stacking) is another ensemble learning method, which introduces a different way of combining multiple base models to achieve better model prediction. The idea behind stacking is fairly simple: instead of using trivial functions (such as hard voting) to aggregate the predictions of the base learners, we train a model to perform this aggregation, called meta learner or blender. This idea is captured in the following diagram.\n",
    "\n",
    "<img src=\"stacking.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "In this lab assignment, you will implement the stacking method and apply it on several machine learning benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# make this notebook's output stable across runs\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions used in this lab\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a learnt classifier\n",
    "    \"\"\"\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=1)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    \n",
    "def evaluate_model(model, X, y):\n",
    "    # make prediction and evaluate the model performance on test data\n",
    "    z_prob = model.predict_proba(X)[:,1]\n",
    "\n",
    "    print(\"model ROC AUC: {}\".format(roc_auc_score(y, z_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Moon Dataset\n",
    "In this lab, we will classify a moon shaped synthetic dataset with two features (_x1_ and _x2_) and two classes (0 or 1) with some added noise. We use this synthetic dataset because it's easier to help demonstrate the pros and cons of different SVM models covered in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the moon train and test data from CSV files\n",
    "train = pd.read_csv(\"moon_train.csv\")\n",
    "test = pd.read_csv(\"moon_test.csv\")\n",
    "\n",
    "train_x = train.iloc[:,0:2]\n",
    "train_y = train.iloc[:,2]\n",
    "\n",
    "test_x = test.iloc[:,0:2]\n",
    "test_y = test.iloc[:,2]\n",
    "\n",
    "print(\"Number of train data: {}\".format(len(train_y)))\n",
    "print(\"Number of test data: {}\".format(len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the train dataset\n",
    "plt.scatter(train_x.x1, train_x.x2, s=40, c=train_y, cmap=plt.cm.Spectral)\n",
    "plt.title('Scatter plot of the train dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the test dataset\n",
    "plt.scatter(test_x.x1, test_x.x2, s=40, c=test_y, cmap=plt.cm.Spectral)\n",
    "plt.title('Scatter plot of the test dataset')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Implementation\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 1: train three base learners separately and evaluate their performance on the test set. We later compare our own implementation of stacking against the performance of these base learners in terms of AUC.**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbors\n",
    "- Train a K nearest neighbors model\n",
    "- Plot the decision boundary\n",
    "- Evaluate the model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: train a KNN model using grid search with cross validation with the following hyper-parameters\n",
    "parameters = {\n",
    "    \"n_neighbors\": [2, 4, 8]\n",
    "}\n",
    "knn_model = # TODO: train a KNN model\n",
    "knn_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(knn_model, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(knn_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear SVM\n",
    "- Train a linear SVM model\n",
    "- Plot the decision boundary\n",
    "- Evaluate the model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: train a linear SVM model using grid search with cross validation with the following hyper-parameters\n",
    "parameters = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "linear_svc_model = # TODO: train a linear SVM model\n",
    "linear_svc_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(linear_svc_model, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(linear_svc_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "- Train a decision tree\n",
    "- Plot the decision boundary\n",
    "- Evaluate the model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: train a decision tree model using grid search with cross validation with the following hyper-parameters\n",
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'], \n",
    "    'max_depth': [1, 2, 4, 8], \n",
    "    'min_samples_split': [0.01, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "}\n",
    "\n",
    "dt_model = # TODO: train a decision tree model\n",
    "dt_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(dt_model, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(dt_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Implementation\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 2: implement the stacking algorithm for a binary classification task.**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-layer Stacking model\n",
    "\n",
    "Given a two-layer stacking architecture with $N$ base learners in the first cell of this notebook, the algorithm works as follows: \n",
    "- Step 1: the training set is split into two subsets: _Fold 1_ and _Fold 2_.\n",
    "- Step 2: the first subset _Fold 1_ is used to train the $N$ base models in the first layer.\n",
    "- Step 3: the first layer base learners are used to make predictions on the second set.\n",
    "- Step 4: for each instance in the second set _Fold 2_, there are $N$ predicted values, one from each base learner. Now, we create a new training set using these predicted values as input features together with the original label as target. \n",
    "- Step 5: train the meta learner (or blender) on this new training dataset so it learns to predict the target value based on the predictions from the base learner in the first layer.\n",
    "\n",
    "Now, given a new instance in the test set, we first use the base learners in the first layer to make predictions, and then use these predictions to feed to the meta learner to get the final prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Multiple-layer Stacking\n",
    "We can actually extend the idea of stacking to more than 2 layers. For example in a 3-layer stacking framework, we can split the training dataset into 3-fold where the first fold is used to train the base learners in the first layers, the second fold is used to train the learners in the middle layer, and the last fold is used to train the final blender. Same as the 2-layer stacking, learners in the middle and last layer use the predictions from learners from the previous layer. Once training is done, we can make a prediction for a new instance by going through each layer sequentially as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a two-layer stacking algorithm with the following components:\n",
    "- Three base learners in the first layer, including Logistic Regression, Support Vector Machine, and Decision Tree. \n",
    "- A logistic regression model as the meta learner or blender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyStacking:\n",
    "    \"\"\"\n",
    "    Stacking\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_knn = None\n",
    "        self.model_svc = None\n",
    "        self.model_dt = None\n",
    "        self.blender = None\n",
    "\n",
    "    def train(self, x, y):\n",
    "        \"\"\"\n",
    "        TODO: train the stacking model with {KNN, SVM, decision tree} as the base learners \n",
    "              and a logistic regression model as the blender\n",
    "        \"\"\"\n",
    "        #--------------------------------------------\n",
    "        # TODO: random split the training set x into two folds\n",
    "        #--------------------------------------------\n",
    "        idx = np.random.choice(x.index, size=len(x)//2, replace=False)\n",
    "        x_fold1 = # TODO\n",
    "        x_fold2 = # TODO\n",
    "        y_fold1 = # TODO\n",
    "        y_fold2 = # TODO\n",
    "        \n",
    "        #--------------------------------------------\n",
    "        # TODO: learn the base learners using data in fold 1\n",
    "        #--------------------------------------------\n",
    "        # learn the KNN base learner\n",
    "        print(\"learn the K nearest neighbor base learner...\")\n",
    "        parameters = {\n",
    "            \"n_neighbors\": [2, 4, 8]\n",
    "        }\n",
    "        self.model_knn = # TODO\n",
    "        self.model_knn.fit(x_fold1, y_fold1)\n",
    "\n",
    "        # learn the SVM base learner\n",
    "        print(\"learn the support vector machine base learner...\")\n",
    "        parameters = {\n",
    "            \"C\": [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "        self.model_svc = # TODO\n",
    "        self.model_svc.fit(x_fold1, y_fold1)\n",
    "\n",
    "        # learn the decision tree base learner\n",
    "        print(\"learn the decision tree base learner...\")\n",
    "        parameters = {\n",
    "            'criterion': ['gini', 'entropy'], \n",
    "            'max_depth': [1, 2, 4, 8], \n",
    "            'min_samples_split': [0.01, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "        }\n",
    "        self.model_dt = # TODO\n",
    "        self.model_dt.fit(x_fold1, y_fold1)\n",
    "\n",
    "        #--------------------------------------------\n",
    "        # TODO: learn the blender using predictions on fold2 from the base learners as features\n",
    "        #--------------------------------------------\n",
    "        # make predictions using the base learns to generate the training data for the blender\n",
    "        x = # TODO\n",
    "        y = # TODO\n",
    "                \n",
    "        print(\"learn the blender...\")\n",
    "        parameters = {\n",
    "            \"penalty\": [\"l1\", \"l2\"], \n",
    "            \"C\": [0.001, 0.01, 0.1, 1, 10]\n",
    "        }\n",
    "        self.blender = # TODO\n",
    "        self.blender.fit(x, y)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Make predictions (probabilities) with the trained stacking model.\n",
    "        \"\"\"\n",
    "        if self.blender == None:\n",
    "            sys.exit(\"The model has not been trained yet. Please call train() first. Exiting...\")\n",
    "\n",
    "        # predictions from the base learner \n",
    "        base_model_predictions = # TODO\n",
    "        \n",
    "        # predictions from the blender\n",
    "        probabilities = # TODO\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Make predictions (the class/label) with the trained stacking model.\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(x)[:,1]\n",
    "        prediction = pd.Series([1] * len(probabilities))\n",
    "        prediction[probabilities < 0.5] = 0\n",
    "\n",
    "        return prediction.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train random forest classifier\n",
    "my_stacking = MyStacking()\n",
    "my_stacking.train(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the decision boundary\n",
    "plot_decision_boundary(my_stacking, train_x.values, train_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and evaluate the model performance on test data\n",
    "evaluate_model(my_stacking, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "Your stacking model should outperform each individual learner by a small margin in terms of AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of ML 310 Lab Assignment 5\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
