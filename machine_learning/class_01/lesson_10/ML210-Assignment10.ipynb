{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 10: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dataset(s) needed: MNIST (\"Modified National Institute of Standards and Technology\") dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#Load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "X = mnist.data / 255.0\n",
    "y = mnist.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Q.1. Split the data into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.2. Train a Logistic Regression classifier on the dataset and see how long it takes.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 104.98s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "log_clf = LogisticRegression(fit_intercept=False, max_iter=1000, solver='lbfgs')\n",
    "start_time = time.time()\n",
    "# Train the classifier\n",
    "log_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.3. Evaluate the resulting model on the test set.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Accuracy Score: 0.9212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = log_clf.predict(X_test)\n",
    "print(\"Logistic Accuracy Score:\", accuracy_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Q.4. Use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.09736019 0.07162769 0.06157279 0.05407583 0.04894241 0.04314663\n",
      " 0.0326955  0.02886339 0.02755206 0.02336354 0.02114186 0.02036159\n",
      " 0.01710273 0.01697588 0.01579852 0.01483028 0.01315072 0.01277798\n",
      " 0.01188548 0.01154643 0.01069553 0.01010967 0.00954102 0.00907833\n",
      " 0.00882614 0.00838996 0.00809334 0.00785285 0.00740609 0.00689452\n",
      " 0.00657504 0.00644894 0.00601529 0.00586087 0.00568734 0.00542785\n",
      " 0.00505607 0.00487531 0.00479006 0.00466511 0.00454422 0.00445376\n",
      " 0.00419137 0.00396211 0.00384115 0.00375532 0.00361444 0.00350354\n",
      " 0.00338201 0.00319514 0.00316586 0.00309288 0.00295258 0.00287322\n",
      " 0.00282207 0.00269456 0.00267291 0.00256465 0.00253613 0.00243878\n",
      " 0.00239702 0.00238198 0.00229797 0.00221263 0.00212635 0.00205955\n",
      " 0.00202272 0.00194566 0.00191948 0.00188817 0.00187128 0.0018004\n",
      " 0.00176297 0.00172727 0.0016457  0.00163152 0.00161328 0.00154714\n",
      " 0.0014698  0.00142147 0.00140752 0.00140012 0.00139287 0.00134772\n",
      " 0.00132494 0.00132162 0.00129191 0.00125004 0.00122779 0.00120392\n",
      " 0.00116065 0.00114774 0.00112996 0.00109798 0.0010824  0.00107118\n",
      " 0.00103468 0.00103278 0.00101043 0.00099794 0.00097628 0.00094498\n",
      " 0.00093388 0.0009105  0.00090107 0.00088786 0.000859   0.00085307\n",
      " 0.00084122 0.00081565 0.00078648 0.00077845 0.00077316 0.00076178\n",
      " 0.00075637 0.00074566 0.00072849 0.00072464 0.00071305 0.00070094\n",
      " 0.00069107 0.00068626 0.00067411 0.00067021 0.00066    0.0006408\n",
      " 0.00062901 0.00062592 0.00061591 0.00059809 0.00059296 0.00058747\n",
      " 0.00057888 0.00057859 0.00057219 0.00056649 0.00055184 0.00054509\n",
      " 0.00052878 0.00052002 0.00051057 0.00049519 0.00049137 0.00048626\n",
      " 0.00048372 0.00047593 0.0004702  0.00046038 0.00045698 0.00044686\n",
      " 0.00043966 0.00043823 0.00043213 0.00042484 0.00041553]\n",
      "Total Explained Ratio: 0.9504281623866676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Grab feature columns\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(X_train.shape[1]) ]\n",
    "df = pd.DataFrame(X_train,columns=feat_cols)\n",
    "df['y'] = y_train\n",
    "df['label'] = df['y'].apply(lambda i: str(i))\n",
    "\n",
    "pca = PCA(n_components=155)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "total = 0\n",
    "for evr in pca.explained_variance_ratio_:\n",
    "    total += evr\n",
    "print(\"Total Explained Ratio:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Q.5. Train a new Logistic Regression classifier on the reduced dataset and see how long it takes. Was training much faster? Explain your results.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 10.82s\n"
     ]
    }
   ],
   "source": [
    "log_mod = LogisticRegression(fit_intercept=False, max_iter=1000, solver='lbfgs')\n",
    "start_time = time.time()\n",
    "# Train the classifier\n",
    "log_mod.fit(pca_result, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))\n",
    "# Training was much faster, nearly 10x faster. The training time I've observed was 106 seconds without PCA, and 11.7\n",
    "# seconds with PCA. The dimensionality reduction was effective in lowering the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.6. Evaluate the new classifier on the test set: how does it compare to the previous classifier? Discuss the speed / accuracy trade-off and in which case you'd prefer a very slight drop in model performance for a x-time speedup in training.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9145\n"
     ]
    }
   ],
   "source": [
    "test_feat_cols = [ 'pixel'+str(i) for i in range(X_test.shape[1]) ]\n",
    "test_df = pd.DataFrame(X_test,columns=test_feat_cols)\n",
    "\n",
    "pca_test = PCA(n_components=155)\n",
    "pca_result_test = pca.transform(test_df[test_feat_cols].values)\n",
    "\n",
    "y_pred_pca = log_mod.predict(pca_result_test)\n",
    "print(accuracy_score(y_true=y_test, y_pred=y_pred_pca))\n",
    "# It seems the accuracy score has been reduced to 91.44%. The previous model (no PCA) was 92.12%.\n",
    "# Given these numbers have been arrived at correctly, I would be willing to use PCA with this reduced accuracy, especially\n",
    "# on a larger dataset where it could take hours/days to fit a model. If the accuracy score was drastically reduced, in this\n",
    "# case maybe around <70%, I would consider keeping the 'slow' model. It really depends on what accuracy score is desired\n",
    "# and the overall computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.7. Create a new text cell in your Notebook: Complete a 50-100 word summary \n",
    "    (or short description of your thinking in applying this week's learning to the solution) \n",
    "     of your experience in this assignment. Include:\n",
    "<br>                                                                    \n",
    "What was your incoming experience with this model, if any?\n",
    "what steps you took, what obstacles you encountered.\n",
    "how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?)\n",
    "This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter summary here\n",
    "# No incoming knowledge. The biggest issue I had was reducing the dimensionality of the test set. I wasn't sure if\n",
    "# I should have done PCA on the entire set and then split it up, or if the way I did it was correct.\n",
    "# The other problem I encountered was when I reduced the dimensionality of the test set, I ran a fit_transform and\n",
    "# received a low accuracy number (12%). Then I read an article that fit_transform should only be used on training data\n",
    "# while transform is to be used on test data as to not by biased. This increased my accuracy drastically to the number\n",
    "# I had expected to see (>90%). I can imagine that on much larger data sets PCA may not only be nice-to-have, but 100%\n",
    "# necessary to fit models within a reasonable timeframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}